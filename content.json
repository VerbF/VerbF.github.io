{"pages":[{"title":"about","text":"联系我Email:xixiangshu704@qq.com","link":"/about/index.html"}],"posts":[{"title":"机器学习笔记（3）逻辑回归","text":"在线性回归中我们需要预测的是连续的值，而在逻辑回归中我们需要预测的是离散值。虽然该算法中有“回归”二字，但是做的是分类的问题。 场景描述线性回归有以下的应用场景，例如邮件的过滤，需要判断一封邮件是否是垃圾邮件。又或是肿瘤的诊断，需要判断肿瘤是良性或是恶性等等。 基本概念应该如何构建一个分类算法呢？先来看看将我们前面所学的线性回归直接应用到分类问题中会怎么样。在肿瘤的例子中，我们选择肿瘤的大小这一特征来预测肿瘤是良性还是恶性。 我们这里定义 $y\\in \\left\\{ 0,1 \\right\\}$ , 0 代表良性肿瘤，1 代表恶性肿瘤。下图中我们有几个正例和反例，且我们已经根据样本拟合出了一条假设函数的直线。我们将输出的阈值设为0.5，当假设函数的输出值大于等于0.5时，我们预测 y = 1 ，当输出值小于0.5时我们预测 y = 0。根据这条直线我们可预测当肿瘤的size大于这一点（y=0.5时，直线上的点）时我们预测 y = 1，小于这一点时我们则预测 y = 0。这样看来这个假设函数可以很好的将正负样本区分开来，线性回归在分类的表现上似乎很不错，但是且慢，让我们看面的例子：这里我们新增加了一个样本点，并且根据这个新的训练集拟合出了一个新的假设函数，该假设函数的图像如上方蓝色直线所示。从这条直线看来，分类的效果并不好，蓝色点左边的预测 y = 0，右边的预测为 y = 1 则它对相当一部分的样本点做出了错误的预测。由此可见线性回归并不适合直接应用到分类的任务中。","link":"/post/machine-learning-AndrewNg-logistic-regression/"},{"title":"机器学习笔记（2）多变量线性回归","text":"本篇介绍了有关在多特征的情况下如何使用线性回归，以及一些相关的技巧。 场景描述在多数时候我们的特征并不会只有一个。在预测房价的例子中，除了房屋的面积之外，房屋的房间数，楼层，房屋的年龄等也可以用于房价的预测。 面积($x_1$) 房间数($x_2$) 楼层($x_3$) 房屋年龄($x_4$) 价格(y) 2101 3 2 20 460 1236 3 1 40 232 1514 2 2 50 315 符号注释 :n : 特征的数量$x^{(i)}$ : 第i个训练样本的特征向量$x^{(i)}_j$ : 第i个训练样本的第j个特征值 例 : $$x^{(2)} = \\begin{bmatrix} 1236 \\\\ 3 \\\\ 1 \\\\ 40 \\end{bmatrix} \\qquad x_3^{(2)} =1$$ 假设函数因为特征的数量增加，假设函数也做出了相应的变化$$h_\\Theta(x) = \\Theta_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n $$为了书写的方便，我们定义一个$\\quad x_0=1 \\quad$ 即 $(x_0^{(i)}=1)$于是我们有 ：$$h_\\Theta(x) = \\Theta_0x_1 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n $$ 为进一步简化这个表达式，我们使用向量的方式来表示 ： $$X=\\begin{bmatrix} x_0\\\\x_1\\\\x_2\\\\x_3\\\\...\\\\x_n \\end{bmatrix} \\qquad \\Theta=\\begin{bmatrix} \\Theta_0\\\\\\Theta_1\\\\\\Theta_2\\\\\\Theta_3\\\\...\\\\\\Theta_n \\end{bmatrix}$$ $$h_\\Theta(x) = \\underbrace{ \\Theta_0x_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n}_{\\Theta^TX} $$ $$\\Huge\\Downarrow $$ $$h_\\Theta(x) = \\Theta^TX$$ 代价函数$$J(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\Theta(x^{(i)}) - y^{(i)})$$ 梯度下降 $$Repeat\\left\\{ \\Theta_j = \\Theta_j - \\alpha\\frac{\\partial}{\\partial\\Theta_j}J(\\Theta_0,...,\\Theta_n) \\right\\} $$ 需要注意的是，这里的$\\Theta_j$需要同步更新 同步更新 异步更新 temp0 = $\\alpha\\frac{\\partial}{\\partial\\Theta_0}J(\\Theta_0,…,\\Theta_n)$ temp1 = $\\alpha\\frac{\\partial}{\\partial\\Theta_1}J(\\Theta_0,…,\\Theta_n)$ $\\Theta_0$ = temp0$\\Theta_1$ = temp1 temp0 = $\\alpha\\frac{\\partial}{\\partial\\Theta_0}J(\\Theta_0,…,\\Theta_n)$ $\\Theta_0$ = temp0 temp1 = $\\alpha\\frac{\\partial}{\\partial\\Theta_1}J(\\Theta_0,…,\\Theta_n)$$\\Theta_1$ = temp1 特征缩放(Feature Scaling)确保特征的数值大小在相似的规模下，这样梯度下降法可以更快的收敛。在做特征缩放时并不需要太精确，只是为了使梯度下降法能更快的收敛。 缩放前 缩放后 $x_1 = size(0-2000 feet^2)$ $x_1 = \\frac{size(feet^{2})}{2000} \\,(0\\leq x_1 \\leq 1)$ $x_2=number \\, of \\, bedrooms(1-5)$ $x_1 = \\frac{num \\, of \\, bedrooms}{5} \\,(0\\leq x_2 \\leq 1)$ 均值归一化(Mean normalization)特征缩放的一种方法$$x_1 \\Rightarrow \\frac{x_1 - \\mu_1}{S_1}$$$\\mu_1$ 表示训练集中特征$x_1$的平均值$S_1$ 表示该特征值的范围（max - min） 多项式回归多项式回归就是用线性回归的方式去拟合更复杂的函数，甚至是非线性的函数。 特征选择如图所示，我们有两个特征，房子的临街宽度和垂直深度。但我们通常使用面积来表示房屋的大小。所以我们可以使用房屋的面积（临街宽度 x 垂直深度）作为一个特征。 拟合多项式对于下图中的数据集，我们继续使用一次函数来拟合的话，效果并不太好。如果使用二次函数来拟合的话，效果可能也不是特别好，因为我们知道，二次函数的图像（图中蓝色的线）在后面是一个下降的趋势，然而现实中房价并不会随着房屋面积的增加而减少。所以这里我们可以使用三次函数（图中绿色的线）来拟合。我们只要做一些简单的修改就可以将线性回归应用到多项式上。$$h_\\Theta(x)=\\Theta_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3$$$$\\huge\\Downarrow$$$$h_\\Theta(x)=\\Theta_0 + \\Theta_1size + \\Theta_2(size)^2 + \\Theta_3(size)^3\\$$我们令$$x_1=(size)\\ x_2=(size)^2\\ x_3=(size)^3$$即可。需要强调的是，在这种情况下特征缩放就显得尤为重要。 检验方法我们如何判断梯度下降法是否正常工作呢？通常可以观察代价函数的值与迭代次数的关系来判断。当梯度下降法正常运行时，如下图所示，随着迭代次数的增加，代价函数的值越来越小，当梯度下降算法迭代60次左右时，代价函数的值几乎不再变化，说明此时算法已经收敛。当出现以下两种情况时，代价函数的值上下震荡，或是逐渐变大，这都说明梯度下降法并没有正常工作。通常出现这两种情况的原因都是学习率 $\\alpha$ 过大。 学习率选择总的来说学习率过小的话，会导致收敛过慢而学习率过大的话，可能导致无法收敛，代价函数 $J(\\Theta)$ 并不会在每次迭代之后都下降。我们可以通过多次试验的方式找出合适的学习率值的大小。另：按照吴恩达老师的推荐，我们可以如下依次选择学习率的大小。… 0.001，0.03，0.1，0.3 … 正规方程正规方程可以让我们再某些情况下，更快的求解出参数 $\\Theta$。假设我们有m个样本，$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(n)},y^{(n)})$ ,n 个特征。我们将单个样本的特征写成向量形式，再将所有的向量转置后，写成矩阵形式。 $$ x_{(i)} = \\begin{bmatrix} x_0^{(i)} \\\\ x_1^{(i)} \\\\ x_3^{(i)} \\\\ ... \\\\ x_n^{(i)} \\end{bmatrix} \\qquad X = \\underbrace{\\begin{bmatrix} ---(x^{(1)})^T---\\\\ ---(x^{(2)})^T---\\\\ ---(x^{(3)})^T---\\\\ ---------\\\\ ---(x^{(m)})^T---\\\\ \\end{bmatrix}}_{m * (n+1)} $$ 接着，只要求解如下这个矩阵表达式，就可得到参数$\\Theta$的值$$\\Theta = (X^TX)^{-1}X^Ty$$在上式中需要对矩阵求逆，那么如果矩阵不可逆呢？一般来说，大部分矩阵都是可逆的，出现了以下两种情况时，会导致矩阵不可逆: 多余特征如下所示，显然$x_1$和$x_2$两个特征是线性相关的，那么这时就会导致矩阵不可逆$$x_1 = size \\ in \\ feet^2 \\ x_2 = size \\ in \\ m^2$$ 太多特征如果我们的特征数量较多，而样本数量较少，造成特征数量大于样本数量，这种情况下也会导致矩阵不可逆。例如，生物信息学的基因芯片数据中常有成千上万个属性，但往往只有几十，上百个样例。 正规方程与梯度下降法比较 梯度下降 正规方程 缺点 需要多次迭代 需要选择学习率 优点 不需要多次迭代 不需要选择学习率 优点 当特征数量n很大时，也能运行的很好 缺点 当特征数量n较大时，速度很慢 吴恩达老师推荐，当n大于10000时选择梯度下降法，小于10000时选择正规方程法。","link":"/post/machine-learning-AndrewNg-multivariate-linear-regression.md/"},{"title":"机器学习笔记（1）单变量线性回归","text":"机器学习中一种经典的算法 场景描述我们有关于房屋面积和房屋价格的数据集，现在想拟合一条直线通过房屋的面积来预测房屋价格。这条直线应该尽可能的符合已有的数据。 概念介绍假设函数这里我们简单的假设该直线的方程为 $$h(x) = \\Theta x$$ 其中x表示房屋的面积，h(x) 表示预测出的房价。有了这个假设函数我们就可以预测房价了。那么参数$\\Theta$应该怎么确定呢？这里我们需要用到代价函数。 代价函数这里先给出代价函数的表达式$$J(\\Theta)=\\frac{1}{2m}\\sum_{i=1}^{m}{({h_\\Theta}({x}^{(i)})-{y}^{(i)})}^{2}$$ 其中 ${x}^{(i)}$表示第i个数据样本中房屋的面积 ${h_\\Theta}({x}^{(i)})$表示使用假设函数预测房屋面积${x}^{(i)}$的得到的房屋价格 ${y}^{(i)}$表示真实的房屋价格 这里我们选择使用均方误差作为衡量预测结果与真实值的偏差。最前面的 $\\frac{1}{2}$ 只是为了计算方便无需在意。 我们所要做的就是改变$\\Theta$的值，使得代价函数J$(\\Theta)$的值最小,当找到一个$\\Theta$使得代价函数的值最小时，我们就确定了参数$\\Theta$。即我们的优化目标：$$minimizeJ(\\Theta)$$ 为什么说我们要找的$\\Theta$会使代价函数取得最小值呢？接下来举例说明。 代价函数详解代价函数与参数$\\Theta$的关系假设我们的数据集中有三个样本点 (1,1) , (2,2) , (3,3)我们可以使用无数条直线来拟合这些样本，但很显然只有当 $\\Theta = 1$时，即 $y=x$ 这条直线有最好预测效果。然后我们将不同的$\\Theta$值带入代价函数，计算其结果： 从图像上可知，当代价函数的图像在最低点时，对应$\\Theta$的值，就是最佳的结果。 通过这个例子不难发现，只要我们求出代价函数的最小值，就可找到我们想要的参数$\\Theta$的值。 那么代价函数的最小值应该怎么求呢？在数学上有许多方法可以解决这个问题，这里我们使用梯度下降法来求代价函数的最小值。 梯度下降法下图是使用梯度下降法求解$\\Theta$的步骤，开始时我们随机赋给$\\Theta$一个初值，重复执行下面的步骤更新$\\Theta$的值。执行一定次数，当$\\Theta$的值基本不再变化时，我们就求出了$\\Theta$的最后结果。 $$\\Theta = \\Theta - \\alpha\\frac{\\partial{J(\\Theta)}}{\\partial\\Theta}$$ 其中关键的步骤是对代价函数求$\\Theta$的偏导，这可以理解为在求某一点的斜率。 当$\\Theta$的值大于最终结果时，$\\Theta$的取值在最终结果的右边，对应点的斜率大于0，即求出的偏导值大于0， $\\Theta$减去一个大于0的数变小。 当$\\Theta$的值小于最终结果时，$\\Theta$的取值在最终结果的左边，对应点的斜率小于0，即求出的偏导值小于0， $\\Theta$减去一个小于0的数后变大。 当$\\Theta$的值越接近最终结果时，导数越接近0，$\\Theta$变化的速度也越慢。 其中 $\\alpha$ 是学习率，它的大小会改变$\\Theta$的改变速度，但取值不能太大，否则会造成$\\Theta$无法收敛。","link":"/post/machine-learning-AndrewNg-univariate-linear-regression/"},{"title":"机器学习（吴恩达）作业（1）","text":"单变量线性回归在这部分的练习中，你将使用单变量的线性回归来预测食品卡车的利润。假设你是a公司的CEO并正在考虑在不同的城市开设一家连锁餐厅。该连锁店已经在多个城市拥有卡车，并且你拥有有关城市的人口和利润的数据。你可以使用这些数据来帮助你选择接下来在那个城市发展。文件ex1data1.txt包含了线性回归问题的数据集，其中第一列是城市人口，第二列是食品卡车在该城市的利润。 绘制数据图像再开始任务之前，我们通过数据可视化来更好的理解数据。因为这个数据集只有两个属性，所以我们可以使用散点图来进行可视化。（我们在现实生活中遇到的很多问题往往是多维的，不能进行二维的绘制） 首先要载入数据,载入数据后我们将其打印在屏幕上1234567print(\"loading data ex1data1.txt...\")ex1data = loadtData('ex1data1.txt')X = ex1data[0]y = ex1data[1]print(X)print(y)print() loadData 函数实现1234567891011121314# 载入数据，返回一个二维的numpy数组，# 第一维是 x轴数据，第二维是 y轴数据def loadtData(file_path): X = np.array([]) Y = np.array([]) for i in open(file_path): # 根据逗号的位置取出数据 x = i[0:i.index(',')-1] y = i[i.index(',')+1:len(i)-1] # 读出的数据是字符串类型，需要转换为浮点类型 X = np.append(X,float(x)) Y = np.append(Y,float(y)) return np.array([X,Y]) 绘制图像接下来我们调用plotData 函数来绘制散点图，顺便设置一下横纵坐标的标题1234x_label = \"Population of City in 10,000s\"#设置横纵坐标的标题y_label = \"Profit in $10,000s\"plotData(X,y,x_label,y_label) plotData函数实现123456# 将数据可视化,使用散点图def plotData(X,y,x_label,y_label): plt.scatter(X,y) plt.xlabel(x_label) plt.ylabel(y_label) plt.show() 绘制出来的结果应该与下图类似 梯度下降在这一部分，你将使用梯度下降法来拟合单变量线性回归中的参数$\\theta$,使其与我们的数据集相符 更新方程线性回归的目标是使代价函数达到最小值$$J(\\Theta)=\\frac{1}{2m}\\sum_{i=1}^{m}{({h_\\Theta}({x}^{(i)})-{y}^{(i)})}^{2}$$假设函数使用下面的线性模型$$h_\\theta(x) = \\theta^Tx = \\theta_0 +\\theta_1x_1$$重新计算模型中参数$\\theta$的值，通过调整参数的值使代价函数的值最小化。我们使用批次梯度下降法来达到目的。在梯度下降法中，每一次迭代都会完成一次更新$$\\theta_j = \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$需要注意的是，所有的$\\theta_j$需要同时更新。梯度下降的每一次更新都会使参数$\\theta_j$更接近最优的值，同时这也会使代价函数的值达到最小。 实现在上面的步骤中，我们已经准备好了线性回归所需的数据。接着，我们给数据增加一个维度，方便我们对参数$\\theta_0$进行优化。我们将参数$\\theta$都初始化为0，并设置学习率为0.01。123456m = len(y)#样本数量X = np.c_[np.ones(m),X]#为X增加一行 值为1theta = np.zeros((2,1),float)#初始化参数theta#一些梯度下降的设置iterations = 1500 #迭代次数alpha = 0.01 #学习率 计算代价函数当我们使用梯度下降法来最小化代价函数的值时，我们可以通过计算代价函数的值来判断是否收敛。我们接下来的任务就是实现computeCost()函数，该函数的功能就是计算代价函数的值。当我们在实现该函数的时候，需要注意变量X，y是矩阵类型的变量，而不是标量。矩阵的行代表了训练集中的样本。一旦你实现了这个函数，我们就使用初始化为0的参数$\\theta$来运行一次该函数。该函数的运行结果应该是32.0712345678910111213# 计算并显示初始的代价值J = computeCost(X,y,theta)print('With theta = [0 ; 0] ');print(\"Cost computed = %f \\n\" % J)print('Expected cost value (approx) 32.07\\n');# 继续测试代价函数theta[0] = -1theta[1] = 2J = computeCost(X, y, theta);print('\\nWith theta = [-1 ; 2]\\nCost computed = %f\\n'% J);print('Expected cost value (approx) 54.24\\n'); computeCost函数实现123456789# 计算代价函数def computeCost(X,y,theta): m = len(y) result = np.dot(X , theta) result = result - y.reshape(97,1) result = np.square(result) result = np.sum( result,axis=0) result = result/(2.0*float(m)) return result 梯度下降接下来你要完成梯度下降的编码。在你的程序中，你要清楚的理解优化目标是什么，什么是需要更新的。你要记住，代价函数的参数是向量$\\theta$,而不是X，y。也就是说，我们需要更新向量$\\theta$的值来是代价函数最小化，而不是改变 X 或 y。如果你不确定的话，参考上面给出的方程，或是视频的课程。我们可以通过观察代价函数的值在每一次更新中是否持续下降，以此来判断梯度下降法是否正常工作。梯度下降在每一次迭代中都会调用computeCost函数，如果你正确实现了computeCost函数和梯度下降，那么你的代价函数的值绝不会增加，并且将会在算法的最后达到一个稳定的值。123456789print('\\nRunning Gradient Descent ...\\n')# 运行梯度下降theta = gradientDescent(X, y, theta, alpha, iterations);# 将theta的值打印到屏幕上print('Theta found by gradient descent:\\n');print('theta_0 : %f \\ntheta_1 : %f'%(theta[0],theta[1])) ;print('Expected theta values (approx)\\n');print(' -3.6303\\n 1.1664\\n\\n'); gradientDescent 函数实现123456789101112131415161718# 梯度下降def gradientDescent(X, y, theta, alpha, iterations): m = len(y) for i in range(0,iterations): theta_0 = theta[0] - alpha * computePartialDerivative(X,y,theta,0) theta_1 = theta[1] - alpha * computePartialDerivative(X,y,theta,1) theta[0] = theta_0 theta[1] = theta_1 print( \"iterations : \",i, \" cost : \",computeCost(X,y,theta)) return theta# 计算偏导数def computePartialDerivative(X , y, theta , num): m = len(y) result = 0 for i in range(0,m): result += (theta[0]*X[i][0] + theta[1]*X[i][1] - y[i])*X[i][num] result /= m return result 当你完成了以上任务时，使用最后得到的参数$\\theta$的值来绘制假设函数的图像，你会看到与下面类似的图1234# 绘制线性拟合的图plt.plot(X[:,1],np.dot(X,theta))plt.scatter(X[:,1],y,c='r')plt.show() 可视化 J($\\theta$)为了更好的理解代价函数 J($\\theta$) , 我们可以将$\\theta_0$和$\\theta_1$的值绘制在二维的网格上。绘图代码12345678910111213141516171819# 绘制三维的图像fig = plt.figure()axes3d = Axes3D(fig)# 指定参数的区间theta0_vals = np.linspace(-10, 10, 100)theta1_vals = np.linspace(-1, 4, 100)# 存储代价函数值的变量初始化J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))# 为代价函数的变量赋值for i in range(0,len(theta0_vals)): for j in range(0,len(theta1_vals)): t = np.zeros((2,1)) t[0] = theta0_vals[i] t[1] = theta1_vals[j] J_vals[i,j] = computeCost(X, y, t)# 下面这句代码不可少，matplotlib还不熟悉，后面填坑theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals) #必须加上这段代码axes3d.plot_surface(theta0_vals,theta1_vals,J_vals, rstride=1, cstride=1, cmap='rainbow')plt.show() 完整代码最后附上完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D# 载入数据，返回一个二维的numpy数组，# 第一维是 x轴数据，第二维是 y轴数据def loadtData(file_path): X = np.array([]) Y = np.array([]) for i in open(file_path): # 根据逗号的位置取出数据 x = i[0:i.index(',')-1] y = i[i.index(',')+1:len(i)-1] # 读出的数据是字符串类型，需要转换为浮点类型 X = np.append(X,float(x)) Y = np.append(Y,float(y)) return np.array([X,Y])# 将数据可视化,使用散点图def plotData(X,y,x_label,y_label): plt.scatter(X,y) plt.xlabel(x_label) plt.ylabel(y_label) plt.show()# 计算代价函数def computeCost(X,y,theta): m = len(y) result = np.dot(X , theta) result = result - y.reshape(97,1) result = np.square(result) result = np.sum( result,axis=0) result = result/(2.0*float(m)) return result# 梯度下降def gradientDescent(X, y, theta, alpha, iterations): m = len(y) for i in range(0,iterations): theta_0 = theta[0] - alpha * computePartialDerivative(X,y,theta,0) theta_1 = theta[1] - alpha * computePartialDerivative(X,y,theta,1) theta[0] = theta_0 theta[1] = theta_1 print( \"iterations : \",i, \" cost : \",computeCost(X,y,theta)) return theta# 计算偏导数def computePartialDerivative(X , y, theta , num): m = len(y) result = 0 for i in range(0,m): result += (theta[0]*X[i][0] + theta[1]*X[i][1] - y[i])*X[i][num] result /= m return result#======================== 绘图 ====================================print(\"loading data ex1data1.txt...\")ex1data = loadtData('ex1data1.txt')X = ex1data[0]y = ex1data[1]print(X)print(y)print()x_label = \"Population of City in 10,000s\"#设置横纵坐标的标题y_label = \"Profit in $10,000s\"#绘图plotData(X,y,x_label,y_label)#======================= 代价函数 和 梯度下降 ======================m = len(y)#样本数量X = np.c_[np.ones(m),X]#为X增加一行 值为1theta = np.zeros((2,1),float)#初始化参数theta#一些梯度下降的设置iterations = 1500 #迭代次数alpha = 0.01 #学习率print(\"Testing the coust function ...\\n\")# 计算并显示初始的代价值J = computeCost(X,y,theta)print('With theta = [0 ; 0] ');print(\"Cost computed = %f \\n\" % J)print('Expected cost value (approx) 32.07\\n');# 继续测试代价函数theta[0] = -1theta[1] = 2J = computeCost(X, y, theta);print('\\nWith theta = [-1 ; 2]\\nCost computed = %f\\n'% J);print('Expected cost value (approx) 54.24\\n');print('\\nRunning Gradient Descent ...\\n')# 运行梯度下降theta = gradientDescent(X, y, theta, alpha, iterations);# 将theta的值打印到屏幕上print('Theta found by gradient descent:\\n');print('theta_0 : %f \\ntheta_1 : %f'%(theta[0],theta[1])) ;print('Expected theta values (approx)\\n');print(' -3.6303\\n 1.1664\\n\\n');# 绘制线性拟合的图plt.plot(X[:,1],np.dot(X,theta))plt.scatter(X[:,1],y,c='r')plt.show()# 绘制三维的图像fig = plt.figure()axes3d = Axes3D(fig)# 指定参数的区间theta0_vals = np.linspace(-10, 10, 100)theta1_vals = np.linspace(-1, 4, 100)# 存储代价函数值的变量初始化J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))# 为代价函数的变量赋值for i in range(0,len(theta0_vals)): for j in range(0,len(theta1_vals)): t = np.zeros((2,1)) t[0] = theta0_vals[i] t[1] = theta1_vals[j] J_vals[i,j] = computeCost(X, y, t)# 下面这句代码不可少，matplotlib还不熟悉，后面填坑theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals) #必须加上这段代码axes3d.plot_surface(theta0_vals,theta1_vals,J_vals, rstride=1, cstride=1, cmap='rainbow')plt.show()","link":"/post/machine-learning-AndrewNg-ex1/"}],"tags":[{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/逻辑回归/"},{"name":"正规方程","slug":"正规方程","link":"/tags/正规方程/"},{"name":"线性回归","slug":"线性回归","link":"/tags/线性回归/"},{"name":"作业","slug":"作业","link":"/tags/作业/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[{"name":"学习","slug":"学习","link":"/categories/学习/"}]}