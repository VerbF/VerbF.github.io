{"pages":[{"title":"about","text":"联系我Email:xixiangshu704@qq.com","link":"/about/index.html"}],"posts":[{"title":"机器学习笔记--多变量线性回归","text":"本篇介绍了有关在多特征的情况下如何使用线性回归，以及一些相关的技巧。 场景描述在多数时候我们的特征并不会只有一个。在预测房价的例子中，除了房屋的面积之外，房屋的房间数，楼层，房屋的年龄等也可以用于房价的预测。 面积($x_1$) 房间数($x_2$) 楼层($x_3$) 房屋年龄($x_4$) 价格(y) 2101 3 2 20 460 1236 3 1 40 232 1514 2 2 50 315 符号注释 :n : 特征的数量$x^{(i)}$ : 第i个训练样本的特征向量$x^{(i)}_j$ : 第i个训练样本的第j个特征值 例 : $$x^{(2)} = \\begin{bmatrix} 1236 \\\\ 3 \\\\ 1 \\\\ 40 \\end{bmatrix} \\qquad x_3^{(2)} =1$$ 假设函数因为特征的数量增加，假设函数也做出了相应的变化$$h_\\Theta(x) = \\Theta_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n $$为了书写的方便，我们定义一个$\\quad x_0=1 \\quad$ 即 $(x_0^{(i)}=1)$于是我们有 ：$$h_\\Theta(x) = \\Theta_0x_1 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n $$ 为进一步简化这个表达式，我们使用向量的方式来表示 ： $$X=\\begin{bmatrix} x_0\\\\x_1\\\\x_2\\\\x_3\\\\...\\\\x_n \\end{bmatrix} \\qquad \\Theta=\\begin{bmatrix} \\Theta_0\\\\\\Theta_1\\\\\\Theta_2\\\\\\Theta_3\\\\...\\\\\\Theta_n \\end{bmatrix}$$ $$h_\\Theta(x) = \\underbrace{ \\Theta_0x_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n}_{\\Theta^TX} $$ $$\\Huge\\Downarrow $$ $$h_\\Theta(x) = \\Theta^TX$$ 代价函数$$J(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\Theta(x^{(i)}) - y^{(i)})$$ 梯度下降 $$Repeat\\left\\{ \\Theta_j = \\Theta_j - \\alpha\\frac{\\partial}{\\partial\\Theta_j}J(\\Theta_0,...,\\Theta_n) \\right\\} $$ 需要注意的是，这里的$\\Theta_j$需要同步更新 同步更新 异步更新 temp0 = $\\alpha\\frac{\\partial}{\\partial\\Theta_0}J(\\Theta_0,…,\\Theta_n)$ temp1 = $\\alpha\\frac{\\partial}{\\partial\\Theta_1}J(\\Theta_0,…,\\Theta_n)$ $\\Theta_0$ = temp0$\\Theta_1$ = temp1 temp0 = $\\alpha\\frac{\\partial}{\\partial\\Theta_0}J(\\Theta_0,…,\\Theta_n)$ $\\Theta_0$ = temp0 temp1 = $\\alpha\\frac{\\partial}{\\partial\\Theta_1}J(\\Theta_0,…,\\Theta_n)$$\\Theta_1$ = temp1 特征缩放(Feature Scaling)确保特征的数值大小在相似的规模下，这样梯度下降法可以更快的收敛。在做特征缩放时并不需要太精确，只是为了使梯度下降法能更快的收敛。 缩放前 缩放后 $x_1 = size(0-2000 feet^2)$ $x_1 = \\frac{size(feet^{2})}{2000} \\,(0\\leq x_1 \\leq 1)$ $x_2=number \\, of \\, bedrooms(1-5)$ $x_1 = \\frac{num \\, of \\, bedrooms}{5} \\,(0\\leq x_2 \\leq 1)$ 均值归一化(Mean normalization)特征缩放的一种方法$$x_1 \\Rightarrow \\frac{x_1 - \\mu_1}{S_1}$$$\\mu_1$ 表示训练集中特征$x_1$的平均值$S_1$ 表示该特征值的范围（max - min） 多项式回归多项式回归就是用线性回归的方式去拟合更复杂的函数，甚至是非线性的函数。 特征选择如图所示，我们有两个特征，房子的临街宽度和垂直深度。但我们通常使用面积来表示房屋的大小。所以我们可以使用房屋的面积（临街宽度 x 垂直深度）作为一个特征。 拟合多项式对于下图中的数据集，我们继续使用一次函数来拟合的话，效果并不太好。如果使用二次函数来拟合的话，效果可能也不是特别好，因为我们知道，二次函数的图像（图中蓝色的线）在后面是一个下降的趋势，然而现实中房价并不会随着房屋面积的增加而减少。所以这里我们可以使用三次函数（图中绿色的线）来拟合。我们只要做一些简单的修改就可以将线性回归应用到多项式上。$$h_\\Theta(x)=\\Theta_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3$$$$\\huge\\Downarrow$$$$h_\\Theta(x)=\\Theta_0 + \\Theta_1size + \\Theta_2(size)^2 + \\Theta_3(size)^3\\$$我们令$$x_1=(size)\\ x_2=(size)^2\\ x_3=(size)^3$$即可。需要强调的是，在这种情况下特征缩放就显得尤为重要。 检验方法我们如何判断梯度下降法是否正常工作呢？通常可以观察代价函数的值与迭代次数的关系来判断。当梯度下降法正常运行时，如下图所示，随着迭代次数的增加，代价函数的值越来越小，当梯度下降算法迭代60次左右时，代价函数的值几乎不再变化，说明此时算法已经收敛。当出现以下两种情况时，代价函数的值上下震荡，或是逐渐变大，这都说明梯度下降法并没有正常工作。通常出现这两种情况的原因都是学习率 $\\alpha$ 过大。 学习率选择总的来说学习率过小的话，会导致收敛过慢而学习率过大的话，可能导致无法收敛，代价函数 $J(\\Theta)$ 并不会在每次迭代之后都下降。我们可以通过多次试验的方式找出合适的学习率值的大小。另：按照吴恩达老师的推荐，我们可以如下依次选择学习率的大小。… 0.001，0.03，0.1，0.3 … 正规方程正规方程可以让我们再某些情况下，更快的求解出参数 $\\Theta$。假设我们有m个样本，$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(n)},y^{(n)})$ ,n 个特征。我们将单个样本的特征写成向量形式，再将所有的向量转置后，写成矩阵形式。 $$ x_{(i)} = \\begin{bmatrix} x_0^{(i)} \\\\ x_1^{(i)} \\\\ x_3^{(i)} \\\\ ... \\\\ x_n^{(i)} \\end{bmatrix} \\qquad X = \\underbrace{\\begin{bmatrix} ---(x^{(1)})^T---\\\\ ---(x^{(2)})^T---\\\\ ---(x^{(3)})^T---\\\\ ---------\\\\ ---(x^{(m)})^T---\\\\ \\end{bmatrix}}_{m * (n+1)} $$ 接着，只要求解如下这个矩阵表达式，就可得到参数$\\Theta$的值$$\\Theta = (X^TX)^{-1}X^Ty$$在上式中需要对矩阵求逆，那么如果矩阵不可逆呢？一般来说，大部分矩阵都是可逆的，出现了以下两种情况时，会导致矩阵不可逆: 多余特征如下所示，显然$x_1$和$x_2$两个特征是线性相关的，那么这时就会导致矩阵不可逆$$x_1 = size \\ in \\ feet^2 \\ x_2 = size \\ in \\ m^2$$ 太多特征如果我们的特征数量较多，而样本数量较少，造成特征数量大于样本数量，这种情况下也会导致矩阵不可逆。例如，生物信息学的基因芯片数据中常有成千上万个属性，但往往只有几十，上百个样例。 正规方程与梯度下降法比较 梯度下降 正规方程 缺点 需要多次迭代 需要选择学习率 优点 不需要多次迭代 不需要选择学习率 优点 当特征数量n很大时，也能运行的很好 缺点 当特征数量n较大时，速度很慢 吴恩达老师推荐，当n大于10000时选择梯度下降法，小于10000时选择正规方程法。","link":"/post/机器学习笔记-多变量线性回归/"},{"title":"机器学习_吴恩达_作业1","text":"单变量线性回归在这部分的练习中，你将使用单变量的线性回归来预测食品卡车的利润。假设你是a公司的CEO并正在考虑在不同的城市开设一家连锁餐厅。该连锁店已经在多个城市拥有卡车，并且你拥有有关城市的人口和利润的数据。你可以使用这些数据来帮助你选择接下来在那个城市发展。文件ex1data1.txt包含了线性回归问题的数据集，其中第一列是城市人口，第二列是食品卡车在该城市的利润。 绘制数据图像再开始任务之前，我们通过数据可视化来更好的理解数据。因为这个数据集只有两个属性，所以我们可以使用散点图来进行可视化。（我们在现实生活中遇到的很多问题往往是多维的，不能进行二维的绘制） 首先要载入数据,载入数据后我们将其打印在屏幕上1234567print(\"loading data ex1data1.txt...\")ex1data = loadtData('ex1data1.txt')X = ex1data[0]y = ex1data[1]print(X)print(y)print() 绘制图像接下来我们调用plotData 函数来绘制散点图，顺便设置一下横纵坐标的标题1234x_label = \"Population of City in 10,000s\"#设置横纵坐标的标题y_label = \"Profit in $10,000s\"plotData(X,y,x_label,y_label) 绘制出来的结果应该与下图类似 梯度下降在这一部分，你将使用梯度下降法来拟合单变量线性回归中的参数$\\theta$,使其与我们的数据集相符 更新方程线性回归的目标是使代价函数达到最小值$$J(\\Theta)=\\frac{1}{2m}\\sum_{i=1}^{m}{({h_\\Theta}({x}^{(i)})-{y}^{(i)})}^{2}$$假设函数使用下面的线性模型$$h_\\theta(x) = \\theta^Tx = \\theta_0 +\\theta_1x_1$$重新计算模型中参数$\\theta$的值，通过调整参数的值使代价函数的值最小化。我们使用批次梯度下降法来达到目的。在梯度下降法中，每一次迭代都会完成一次更新$$\\theta_j = \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$需要注意的是，所有的$\\theta_j$需要同时更新。梯度下降的每一次更新都会使参数$\\theta_j$更接近最优的值，同时这也会使代价函数的值达到最小。 实现在上面的步骤中，我们已经准备好了线性回归所需的数据。接着，我们给数据增加一个维度，方便我们对参数$\\theta_0$进行优化。我们将参数$\\theta$都初始化为0，并设置学习率为0.01。123456m = len(y)#样本数量X = np.c_[np.ones(m),X]#为X增加一行 值为1theta = np.zeros((2,1),float)#初始化参数theta#一些梯度下降的设置iterations = 1500 #迭代次数alpha = 0.01 #学习率 计算代价函数当我们使用梯度下降法来最小化代价函数的值时，我们可以通过计算代价函数的值来判断是否收敛。我们接下来的任务就是实现computeCost()函数，该函数的功能就是计算代价函数的值。当我们在实现该函数的时候，需要注意变量X，y是矩阵类型的变量，而不是标量。矩阵的行代表了训练集中的样本。一旦你实现了这个函数，我们就使用初始化为0的参数$\\theta$来运行一次该函数。该函数的运行结果应该是32.0712345678910111213# 计算并显示初始的代价值J = computeCost(X,y,theta)print('With theta = [0 ; 0] ');print(\"Cost computed = %f \\n\" % J)print('Expected cost value (approx) 32.07\\n');# 继续测试代价函数theta[0] = -1theta[1] = 2J = computeCost(X, y, theta);print('\\nWith theta = [-1 ; 2]\\nCost computed = %f\\n'% J);print('Expected cost value (approx) 54.24\\n'); 梯度下降接下来你要完成梯度下降的编码。在你的程序中，你要清楚的理解优化目标是什么，什么是需要更新的。你要记住，代价函数的参数是向量$\\theta$,而不是X，y。也就是说，我们需要更新向量$\\theta$的值来是代价函数最小化，而不是改变 X 或 y。如果你不确定的话，参考上面给出的方程，或是视频的课程。我们可以通过观察代价函数的值在每一次更新中是否持续下降，以此来判断梯度下降法是否正常工作。梯度下降在每一次迭代中都会调用computeCost函数，如果你正确实现了computeCost函数和梯度下降，那么你的代价函数的值绝不会增加，并且将会在算法的最后达到一个稳定的值。12345678910print('\\nRunning Gradient Descent ...\\n')# 运行梯度下降result = gradientDescent(X, y, theta, alpha, iterations);theta = result[0]# 将theta的值打印到屏幕上print('Theta found by gradient descent:\\n');print('theta_0 : %f \\ntheta_1 : %f'%(theta[0],theta[1])) ;print('Expected theta values (approx)\\n');print(' -3.6303\\n 1.1664\\n\\n'); 当你完成了以上任务时，使用最后得到的参数$\\theta$的值来绘制假设函数的图像，你会看到与下面类似的图","link":"/post/machine-learning-Andrew-Ng-ex1/"},{"title":"机器学习笔记--单变量线性回归","text":"机器学习中一种经典的算法 场景描述我们有关于房屋面积和房屋价格的数据集，现在想拟合一条直线通过房屋的面积来预测房屋价格。这条直线应该尽可能的符合已有的数据。 概念介绍假设函数这里我们简单的假设该直线的方程为 $$h(x) = \\Theta x$$ 其中x表示房屋的面积，h(x) 表示预测出的房价。有了这个假设函数我们就可以预测房价了。那么参数$\\Theta$应该怎么确定呢？这里我们需要用到代价函数。 代价函数这里先给出代价函数的表达式$$J(\\Theta)=\\frac{1}{2m}\\sum_{i=1}^{m}{({h_\\Theta}({x}^{(i)})-{y}^{(i)})}^{2}$$ 其中 ${x}^{(i)}$表示第i个数据样本中房屋的面积 ${h_\\Theta}({x}^{(i)})$表示使用假设函数预测房屋面积${x}^{(i)}$的得到的房屋价格 ${y}^{(i)}$表示真实的房屋价格 这里我们选择使用均方误差作为衡量预测结果与真实值的偏差。最前面的 $\\frac{1}{2}$ 只是为了计算方便无需在意。 我们所要做的就是改变$\\Theta$的值，使得代价函数J$(\\Theta)$的值最小,当找到一个$\\Theta$使得代价函数的值最小时，我们就确定了参数$\\Theta$。即我们的优化目标：$$minimizeJ(\\Theta)$$ 为什么说我们要找的$\\Theta$会使代价函数取得最小值呢？接下来举例说明。 代价函数详解代价函数与参数$\\Theta$的关系假设我们的数据集中有三个样本点 (1,1) , (2,2) , (3,3)我们可以使用无数条直线来拟合这些样本，但很显然只有当 $\\Theta = 1$时，即 $y=x$ 这条直线有最好预测效果。然后我们将不同的$\\Theta$值带入代价函数，计算其结果： 从图像上可知，当代价函数的图像在最低点时，对应$\\Theta$的值，就是最佳的结果。 通过这个例子不难发现，只要我们求出代价函数的最小值，就可找到我们想要的参数$\\Theta$的值。 那么代价函数的最小值应该怎么求呢？在数学上有许多方法可以解决这个问题，这里我们使用梯度下降法来求代价函数的最小值。 梯度下降法下图是使用梯度下降法求解$\\Theta$的步骤，开始时我们随机赋给$\\Theta$一个初值，重复执行下面的步骤更新$\\Theta$的值。执行一定次数，当$\\Theta$的值基本不再变化时，我们就求出了$\\Theta$的最后结果。 $$\\Theta = \\Theta - \\alpha\\frac{\\partial{J(\\Theta)}}{\\partial\\Theta}$$ 其中关键的步骤是对代价函数求$\\Theta$的偏导，这可以理解为在求某一点的斜率。 当$\\Theta$的值大于最终结果时，$\\Theta$的取值在最终结果的右边，对应点的斜率大于0，即求出的偏导值大于0， $\\Theta$减去一个大于0的数变小。 当$\\Theta$的值小于最终结果时，$\\Theta$的取值在最终结果的左边，对应点的斜率小于0，即求出的偏导值小于0， $\\Theta$减去一个小于0的数后变大。 当$\\Theta$的值越接近最终结果时，导数越接近0，$\\Theta$变化的速度也越慢。 其中 $\\alpha$ 是学习率，它的大小会改变$\\Theta$的改变速度，但取值不能太大，否则会造成$\\Theta$无法收敛。","link":"/post/机器学习笔记--单变量线性回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"正规方程","slug":"正规方程","link":"/tags/正规方程/"},{"name":"线性回归","slug":"线性回归","link":"/tags/线性回归/"},{"name":"作业","slug":"作业","link":"/tags/作业/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[{"name":"学习","slug":"学习","link":"/categories/学习/"}]}