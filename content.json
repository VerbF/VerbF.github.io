{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"机器学习笔记--多变量线性回归","text":"本篇介绍了有关在多特征的情况下如何使用线性回归，以及一些相关的技巧。 场景描述在多数时候我们的特征并不会只有一个。在预测房价的例子中，除了房屋的面积之外，房屋的房间数，楼层，房屋的年龄等也可以用于房价的预测。 面积($x_1$) 房间数($x_2$) 楼层($x_3$) 房屋年龄($x_4$) 价格(y) 2101 3 2 20 460 1236 3 1 40 232 1514 2 2 50 315 符号注释 :n : 特征的数量$x^{(i)}$ : 第i个训练样本的特征向量$x^{(i)}_j$ : 第i个训练样本的第j个特征值 例 : $$x^{(2)} = \\begin{bmatrix} 1236 \\\\ 3 \\\\ 1 \\\\ 40 \\end{bmatrix} \\qquad x_3^{(2)} =1$$ 假设函数因为特征的数量增加，假设函数也做出了相应的变化$$h_\\Theta(x) = \\Theta_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n $$为了书写的方便，我们定义一个$\\quad x_0=1 \\quad$ 即 $(x_0^{(i)}=1)$于是我们有 ：$$h_\\Theta(x) = \\Theta_0x_1 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n $$ 为进一步简化这个表达式，我们使用向量的方式来表示 ： $$X=\\begin{bmatrix} x_0\\\\x_1\\\\x_2\\\\x_3\\\\...\\\\x_n \\end{bmatrix} \\qquad \\Theta=\\begin{bmatrix} \\Theta_0\\\\\\Theta_1\\\\\\Theta_2\\\\\\Theta_3\\\\...\\\\\\Theta_n \\end{bmatrix}$$ $$h_\\Theta(x) = \\underbrace{ \\Theta_0x_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3 + … + \\Theta_nx_n}_{\\Theta^TX} $$ $$\\Huge\\Downarrow $$ $$h_\\Theta(x) = \\Theta^TX$$ 代价函数$$J(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\Theta(x^{(i)}) - y^{(i)})$$ 梯度下降 $$Repeat\\left\\{ \\Theta_j = \\Theta_j - \\alpha\\frac{\\partial}{\\partial\\Theta_j}J(\\Theta_0,...,\\Theta_n) \\right\\} $$ 需要注意的是，这里的$\\Theta_j$需要同步更新 同步更新 异步更新 temp0 = $\\alpha\\frac{\\partial}{\\partial\\Theta_0}J(\\Theta_0,…,\\Theta_n)$ temp1 = $\\alpha\\frac{\\partial}{\\partial\\Theta_1}J(\\Theta_0,…,\\Theta_n)$ $\\Theta_0$ = temp0$\\Theta_1$ = temp1 temp0 = $\\alpha\\frac{\\partial}{\\partial\\Theta_0}J(\\Theta_0,…,\\Theta_n)$ $\\Theta_0$ = temp0 temp1 = $\\alpha\\frac{\\partial}{\\partial\\Theta_1}J(\\Theta_0,…,\\Theta_n)$$\\Theta_1$ = temp1 特征缩放(Feature Scaling)确保特征的数值大小在相似的规模下，这样梯度下降法可以更快的收敛。在做特征缩放时并不需要太精确，只是为了使梯度下降法能更快的收敛。 缩放前 缩放后 $x_1 = size(0-2000 feet^2)$ $x_1 = \\frac{size(feet^{2})}{2000} \\,(0\\leq x_1 \\leq 1)$ $x_2=number \\, of \\, bedrooms(1-5)$ $x_1 = \\frac{num \\, of \\, bedrooms}{5} \\,(0\\leq x_2 \\leq 1)$ 均值归一化(Mean normalization)特征缩放的一种方法$$x_1 \\Rightarrow \\frac{x_1 - \\mu_1}{S_1}$$$\\mu_1$ 表示训练集中特征$x_1$的平均值$S_1$ 表示该特征值的范围（max - min） 多项式回归多项式回归就是用线性回归的方式去拟合更复杂的函数，甚至是非线性的函数。 特征选择如图所示，我们有两个特征，房子的临街宽度和垂直深度。但我们通常使用面积来表示房屋的大小。所以我们可以使用房屋的面积（临街宽度 x 垂直深度）作为一个特征。 拟合多项式对于下图中的数据集，我们继续使用一次函数来拟合的话，效果并不太好。如果使用二次函数来拟合的话，效果可能也不是特别好，因为我们知道，二次函数的图像（图中蓝色的线）在后面是一个下降的趋势，然而现实中房价并不会随着房屋面积的增加而减少。所以这里我们可以使用三次函数（图中绿色的线）来拟合。我们只要做一些简单的修改就可以将线性回归应用到多项式上。$$h_\\Theta(x)=\\Theta_0 + \\Theta_1x_1 + \\Theta_2x_2 + \\Theta_3x_3$$$$\\huge\\Downarrow$$$$h_\\Theta(x)=\\Theta_0 + \\Theta_1size + \\Theta_2(size)^2 + \\Theta_3(size)^3\\$$我们令$$x_1=(size)\\ x_2=(size)^2\\ x_3=(size)^3$$即可。需要强调的是，在这种情况下特征缩放就显得尤为重要。 检验方法我们如何判断梯度下降法是否正常工作呢？通常可以观察代价函数的值与迭代次数的关系来判断。当梯度下降法正常运行时，如下图所示，随着迭代次数的增加，代价函数的值越来越小，当梯度下降算法迭代60次左右时，代价函数的值几乎不再变化，说明此时算法已经收敛。当出现以下两种情况时，代价函数的值上下震荡，或是逐渐变大，这都说明梯度下降法并没有正常工作。通常出现这两种情况的原因都是学习率 $\\alpha$ 过大。 学习率选择总的来说学习率过小的话，会导致收敛过慢而学习率过大的话，可能导致无法收敛，代价函数 $J(\\Theta)$ 并不会在每次迭代之后都下降。我们可以通过多次试验的方式找出合适的学习率值的大小。另：按照吴恩达老师的推荐，我们可以如下依次选择学习率的大小。… 0.001，0.03，0.1，0.3 … 正规方程正规方程可以让我们再某些情况下，更快的求解出参数 $\\Theta$。假设我们有m个样本，$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(n)},y^{(n)})$ ,n 个特征。我们将单个样本的特征写成向量形式，再将所有的向量转置后，写成矩阵形式。 $$ x_{(i)} = \\begin{bmatrix} x_0^{(i)} \\\\ x_1^{(i)} \\\\ x_3^{(i)} \\\\ ... \\\\ x_n^{(i)} \\end{bmatrix} \\qquad X = \\underbrace{\\begin{bmatrix} ---(x^{(1)})^T---\\\\ ---(x^{(2)})^T---\\\\ ---(x^{(3)})^T---\\\\ ---------\\\\ ---(x^{(m)})^T---\\\\ \\end{bmatrix}}_{m * (n+1)} $$ 接着，只要求解如下这个矩阵表达式，就可得到参数$\\Theta$的值$$\\Theta = (X^TX)^{-1}X^Ty$$在上式中需要对矩阵求逆，那么如果矩阵不可逆呢？一般来说，大部分矩阵都是可逆的，出现了以下两种情况时，会导致矩阵不可逆: 多余特征如下所示，显然$x_1$和$x_2$两个特征是线性相关的，那么这时就会导致矩阵不可逆$$x_1 = size \\ in \\ feet^2 \\ x_2 = size \\ in \\ m^2$$ 太多特征如果我们的特征数量较多，而样本数量较少，造成特征数量大于样本数量，这种情况下也会导致矩阵不可逆。例如，生物信息学的基因芯片数据中常有成千上万个属性，但往往只有几十，上百个样例。 正规方程与梯度下降法比较 梯度下降 正规方程 缺点 需要多次迭代 需要选择学习率 优点 不需要多次迭代 不需要选择学习率 优点 当特征数量n很大时，也能运行的很好 缺点 当特征数量n较大时，速度很慢 吴恩达老师推荐，当n大于10000时选择梯度下降法，小于10000时选择正规方程法。","link":"/post/机器学习笔记-多变量线性回归/"},{"title":"机器学习笔记--单变量线性回归","text":"机器学习中一种经典的算法 场景描述我们有关于房屋面积和房屋价格的数据集，现在想拟合一条直线通过房屋的面积来预测房屋价格。这条直线应该尽可能的符合已有的数据。 概念介绍假设函数这里我们简单的假设该直线的方程为 $$h(x) = \\Theta x$$ 其中x表示房屋的面积，h(x) 表示预测出的房价。有了这个假设函数我们就可以预测房价了。那么参数$\\Theta$应该怎么确定呢？这里我们需要用到代价函数。 代价函数这里先给出代价函数的表达式$$J(\\Theta)=\\frac{1}{2m}\\sum_{i=1}^{m}{({h_\\Theta}({x}^{(i)})-{y}^{(i)})}^{2}$$ 其中 ${x}^{(i)}$表示第i个数据样本中房屋的面积 ${h_\\Theta}({x}^{(i)})$表示使用假设函数预测房屋面积${x}^{(i)}$的得到的房屋价格 ${y}^{(i)}$表示真实的房屋价格 这里我们选择使用均方误差作为衡量预测结果与真实值的偏差。最前面的 $\\frac{1}{2}$ 只是为了计算方便无需在意。 我们所要做的就是改变$\\Theta$的值，使得代价函数J$(\\Theta)$的值最小,当找到一个$\\Theta$使得代价函数的值最小时，我们就确定了参数$\\Theta$。即我们的优化目标：$$minimizeJ(\\Theta)$$ 为什么说我们要找的$\\Theta$会使代价函数取得最小值呢？接下来举例说明。 代价函数详解代价函数与参数$\\Theta$的关系假设我们的数据集中有三个样本点 (1,1) , (2,2) , (3,3)我们可以使用无数条直线来拟合这些样本，但很显然只有当 $\\Theta = 1$时，即 $y=x$ 这条直线有最好预测效果。然后我们将不同的$\\Theta$值带入代价函数，计算其结果： 从图像上可知，当代价函数的图像在最低点时，对应$\\Theta$的值，就是最佳的结果。 通过这个例子不难发现，只要我们求出代价函数的最小值，就可找到我们想要的参数$\\Theta$的值。 那么代价函数的最小值应该怎么求呢？在数学上有许多方法可以解决这个问题，这里我们使用梯度下降法来求代价函数的最小值。 梯度下降法下图是使用梯度下降法求解$\\Theta$的步骤，开始时我们随机赋给$\\Theta$一个初值，重复执行下面的步骤更新$\\Theta$的值。执行一定次数，当$\\Theta$的值基本不再变化时，我们就求出了$\\Theta$的最后结果。 $$\\Theta = \\Theta - \\alpha\\frac{\\partial{J(\\Theta)}}{\\partial\\Theta}$$ 其中关键的步骤是对代价函数求$\\Theta$的偏导，这可以理解为在求某一点的斜率。 当$\\Theta$的值大于最终结果时，$\\Theta$的取值在最终结果的右边，对应点的斜率大于0，即求出的偏导值大于0， $\\Theta$减去一个大于0的数变小。 当$\\Theta$的值小于最终结果时，$\\Theta$的取值在最终结果的左边，对应点的斜率小于0，即求出的偏导值小于0， $\\Theta$减去一个小于0的数后变大。 当$\\Theta$的值越接近最终结果时，导数越接近0，$\\Theta$变化的速度也越慢。 其中 $\\alpha$ 是学习率，它的大小会改变$\\Theta$的改变速度，但取值不能太大，否则会造成$\\Theta$无法收敛。","link":"/post/机器学习笔记--单变量线性回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"正规方程","slug":"正规方程","link":"/tags/正规方程/"},{"name":"线性回归","slug":"线性回归","link":"/tags/线性回归/"}],"categories":[{"name":"学习","slug":"学习","link":"/categories/学习/"}]}